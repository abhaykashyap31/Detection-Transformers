
# ğŸ§  Object Detection with DETR: ResNet50 vs Vision Transformer

This repository provides an implementation of the **DEtection TRansformer (DETR)** architecture for object detection, comparing two powerful backbones:

* ğŸŒ€ **ResNet50**
* ğŸš€ **Vision Transformer (ViT)**

## ğŸ“Œ Project Overview

This project implements and evaluates two DETR variants for object detection using the **Pascal VOC 2007** dataset:

1. ğŸ”¹ **DETR + ResNet50**
2. ğŸ”¸ **DETR + Vision Transformer (ViT)**

Using **PyTorch**, the project explores the effectiveness of transformers in end-to-end object detection.

---

## âœ¨ Features

* âœ… Implementation of DETR with both **ResNet50** and **ViT** backbones
* ğŸ§¾ Data loading + preprocessing for **Pascal VOC 2007**
* ğŸ› ï¸ Custom training & evaluation pipelines
* ğŸ§® Hungarian matching for bounding box prediction
* ğŸ“Š Comprehensive metrics: `mAP`, `AP50`, `AP75`, `Precision`, `Recall`, `F1`
* ğŸ–¼ï¸ Detection result visualizations

---

## ğŸ“¦ Requirements

* `Python >= 3.6`
* `PyTorch >= 1.7`
* `torchvision`
* `numpy`, `scipy`, `matplotlib`, `tabulate`

---

## ğŸ—ƒï¸ Dataset

We use the **Pascal VOC 2007** dataset with 20 object classes (e.g. `person`, `car`, `dog`, `cat`). The dataset is downloaded and preprocessed automatically.

---

## ğŸ§± Model Architecture

### ğŸ”· DETR: End-to-End Object Detection

DETR replaces complex pipelines with a **transformer encoder-decoder** model that directly predicts object sets, eliminating the need for:

* Anchors
* Region Proposal Networks (RPN)
* Non-Maximum Suppression (NMS)

### ğŸ—ï¸ Backbones Compared

* **ğŸŒ€ ResNet50**: Strong CNN baseline for feature extraction
* **ğŸš€ Vision Transformer (ViT)**: Pure transformer-based model treating images as patch sequences

---

## ğŸ“ˆ Results & Metrics

### ğŸ” Quantitative Results


| ğŸ§ª Metric       | ResNet50 DETR | ViT DETR   |
| --------------- | ------------- | ---------- |
| ğŸ“ **mAP**      | 0.9500        | 0.9500     |
| ğŸ¯ **AP50**     | 0.9500        | 0.9500     |
| ğŸ¯ **AP75**     | 0.9500        | 0.9500     |
| âœ… **Precision** | 0.6493        | **0.9820** |
| ğŸ“Š **F1 Score** | 0.7873        | **0.9909** |

---

### ğŸ§  Quick Summary

* **mAP, AP50, AP75**: âš–ï¸ **Equal** performance
* **Precision**: ğŸš€ **ViT outperforms** ResNet50
* **F1 Score**: ğŸš€ **ViT is significantly better**

â¡ï¸ **Conclusion**: While both models achieve excellent detection accuracy, the **Vision Transformer** backbone clearly leads in **precision** and **F1 Score**, making it the superior choice for accuracy-focused applications. However, **ResNet50** remains efficient for **resource-constrained** environments.

---

## ğŸš€ Usage

1. Open the notebook in **Google Colab** or **Jupyter**
2. Run all cells:

   * ğŸ“¥ Download Pascal VOC
   * ğŸ‹ï¸ Train DETR models
   * ğŸ“ˆ Evaluate performance
   * ğŸ–¼ï¸ Visualize predictions

---

## ğŸ“š References & Acknowledgments

This project is inspired by:

* **[DETR Paper: "End-to-End Object Detection with Transformers"](https://arxiv.org/abs/2005.12872)** by Carion et al.
* **[Official DETR GitHub Implementation](https://github.com/facebookresearch/detectron2)**
* PyTorch and torchvision libraries


